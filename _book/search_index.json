[
["matrix-ols-and-non-parametric-bootstrapping-npbs.html", "Chapter7 Matrix OLS and Non-Parametric Bootstrapping (NPBS) 7.1 행렬? 벡터? 전치행렬? 역행렬?", " Chapter7 Matrix OLS and Non-Parametric Bootstrapping (NPBS) 최소자승법(Ordinary least squared)을 이용한 선형회귀분석, 이른바 OLS에서 BLUE란 무엇을 의미하는가? 그리고 표집분포(sampling distribution)란 무엇인가? 모델에 부적절한 변수를 포함시켰을 때, 또는 적절한 변수를 누락시켰을 때 나타날 수 있는 문제들은 무엇이 있는가? 이제는 이와 같은 질문들에 대해 어느 정도 답을 하실 수 있으리라 생각합니다. 또, 아래의 두 선형회귀모델에 대해서 해석할 수 있게 되셨으리라고 생각합니다. \\(y = \\alpha + \\beta_1x + \\beta_2z + \\beta_3xz\\) \\(y - \\alpha + \\beta_1I(x = 1) + \\beta_2I(x = 2)\\), 이고 이때 \\(x \\in\\{0,1,2\\}\\). 마지막으로 가설검정(hypothesis testing)의 기본 논리에 대해서도 조금은 익숙해졌을 것입니다. 어째서 우리는 연구가설(혹은 대안가설)이 맞다라고 직접적으로 검증하지 못하고 경험적 근거를 통해 영가설을 기각하는 “약간은 헷갈릴법한” 방법을 사용하는지 말입니다. 이 챕터에서는 OLS를 다시 한 번 다룰 건데요, 이전과는 다르게 행렬(matrix)을 통해 접근해보고자 합니다. 고등학교 때였나, 행렬이 교과과정에 포함되어 있어 의무적으로 다루었던 것 같기는 한데 혹시 모르니 처음부터 차근차근 살펴보도록 하겠습니다. 7.1 행렬? 벡터? 전치행렬? 역행렬? 저도 OLS 기본을 마무리하고 좀 더 깊게 들어갔을 때, 갑자기 [오랜만에] 등장한 이 행렬 때문에 당황했습니다. 하지만 R로 행렬들이 계산되면서 변화하는 것을 추적하며 공부하면 예전에 수학의 정석 붙잡고 노트에 필기하며 낑낑대었던 것보다는 훨씬 편하게 이해하실 수 있을 겁니다. 대체 행렬(matrix), 벡터(vector), 전치행렬(transposed matrix), 역행렬(inversed matrix)라는 이놈들이 왜 OLS를 공부하는 데 등장하는 것일까요? 그리고 OLS에서 \\(X\\beta\\)가 제대로 돌아가기 위해서 가정되어야 하는 것들은 무엇이 있을까요? 마지막으로 이것들을 R로 어떻게 보여줄 수 있을까요? 참고로 \\(X\\beta\\)는 OLS를 통해 얻은 일련의 계수값을 \\(\\beta\\)라는 하나의 집단으로 나타내고 \\(X\\)도 마찬가지로 모델에 포함된 설명변수 일체를 표시한 것입니다. 행렬과 벡터를 정의하기에 앞서, 우리는 스칼라(scalar)라는 것에 대해서 알아야만 합니다. 저도 정치학을 전공한 사람이고 방법론은 연구문제를 풀기 위한 도구로 공부하는 사람이기 때문에 엄청 깊은 증명이나 디테일에 집착하지는 않겠습니다. 저도 지금 큰 문제없이 연구방법을 통해 연구문제들을 풀어가고 있으니, 제가 이해한 정도와 비슷한 정도로만 이해하셔도 연구에는 큰 지장은 없으실겁니다. 물론 주정공이나 부전공이 방법론이라던가 하는 경우에는 조금 얘기가 달라지겠죠? 어디까지나 부외자에 한정된 이야기입니다. 스칼라는 하나의 차원에서 측정된 수치화된 결과를 말합니다. 따라서 스칼라는 방향에 대한 정보는 가지지 않고 크기(magnitude)에 대한 정보만 가지고 있다고 할 수 있습니다. 벡터는 이런 스칼라들의 집합입니다. 스칼라와는 다르게 벡터는 크기뿐아니라 방향에 대한 정보도 가지고 있습니다. 예를 들면, \\(n\\)개의 스칼라를 가진 \\(a\\)라는 벡터가 있다고 해보겠습니다. 우리는 이 벡터를 \\(n\\)차원의 벡터라고 부를 수 있습니다. 이런 벡터는 아래의 수식 과 같이 열 또는 행으로 나타낼 수 있습니다. \\[\\begin{equation} \\label{eq:1} a = \\begin{bmatrix}a_1 \\\\ a_2 \\\\ a_3 \\\\ \\vdots \\\\ a_n\\end{bmatrix} \\text{ or } a = \\begin{bmatrix}a_1, a_2, a_3, \\cdots, a_n\\end{bmatrix} \\end{equation}\\] 그렇다면 행렬은 무엇일까요? 행렬은 행과 열로 요소들(숫자일 수도 있고 일종의 함수들일 수도 있습니다)이 사각형의 형태를 띄게끔 배열된 결과물입니다. 행렬이라는 이름이 행(raw)과 열(column)이 합쳐진 것이니까 이 정도는 쉽게 유추할 수 있습니다. 행렬을 구성하는 행과 열의 수를 가지고 우리는 행렬의 차원(dimension)이 어떻게 되는지를 말할 수 있습니다. 예를 들면, 아래의 수식 는 요소들의 배열, 즉 행렬의 모습을 보여주고 있습니다. \\(j\\)는 행을, \\(k\\)는 열을 의미하는데요, \\(a_{jk}\\)는 \\(j\\)-행과 \\(k\\)-열에 위치한 요소를 나타냅니다. \\[\\begin{equation} \\label{eq:2} A = \\begin{bmatrix}a_{jk}\\end{bmatrix} = \\begin{bmatrix} a_{11} &amp; a_{12} &amp; \\cdots &amp; a_{1n} \\\\ a_{21} &amp; a_{21} &amp; \\cdots &amp; a_{2n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{m1} &amp; a_{m2} &amp; \\cdots &amp; a_{mn} \\\\ \\end{bmatrix} \\end{equation}\\] 행렬의 정의에 따르면, 우리는 벡터 역시도 일종의 행렬이라고 이해할 수 있습니다. 단 하나의 행(1\\(\\times n\\)) 또는 열(\\(m \\times1\\))로 구성된 행렬인 거죠. 그리고 행렬을 전치(transposition)한다는 것은 말 그대로 위치(position)를 뒤바꾸는 것(trans)이기 때문에 행렬의 행과 열을 서로 뒤바꾼다는 것을 의미합니다(수식 ). 예를 들면 2개의 행과 3개의 열을 가지고 있던 \\(2\\times3\\) 행렬이 3개의 행과 2개의 열을 가진 \\(3\\times 2\\)의 행렬로 전치되는 것입니다. \\[\\begin{equation} \\label{eq:3} A = \\begin{bmatrix}a_{jk}\\end{bmatrix} \\Rightarrow A^T = \\begin{bmatrix}a_{kj}\\end{bmatrix} = \\begin{bmatrix} a_{11} &amp; a_{21} &amp; \\cdots &amp; a_{m1} \\\\ a_{12} &amp; a_{22} &amp; \\cdots &amp; a_{m2} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{1n} &amp; a_{2n} &amp; \\cdots &amp; a_{mn} \\\\ \\end{bmatrix} \\end{equation}\\] 주어진 행렬의 곱셈에 대한 역원을 역행렬이라고 합니다.모든 행렬이 반드시 그 역행렬을 가지는 것은 아닙니다. A라는 행렬이 역행렬을 가진다고 한다면, 우리는 A라는 행렬이 “역행렬을 가질 수 있는 행렬”이라는 의미에서 가역행렬(invertible matrix)라고 부릅니다. 반면에 역행렬을 가지지 않는 행렬을 특이행렬(singular matrix)라고 합니다. 만약에 어떤 행렬이 역행렬을 가진다면, 이때 그 역행렬은 가역행렬에 대해 유일한 역행렬입니다. 1:1 관계가 성립한다는 거죠. A라는 행렬에 대한 역행렬을 우리는 A\\(^{-1}\\)라고 표기합니다. 우리는 행렬을 이용해서 단선순형회귀모델을 표현할 수 있습니다. 이때 필요한 것은 \\(x\\)와 \\(y\\)로, 실제 관측치 \\(x_i\\)를 요소로 하는 예측변수의 행렬이 모델의 계수값이라고 할 수 있는 행렬을 거치게 되면 \\(\\hat{y}_i\\)의 행렬을 산출하는 논리입니다. 마찬가지로 다중선형회귀모델도 행렬로 표현할 수 있습니다. 그러나 이때에는 \\(y_i\\)의 변화를 설명하기 위해 두 개 이상의 예측변수들을 나타내는 행렬이 필요합니다. 즉, 최소 세 차원 이상의 행렬이 필요하다는 것입니다. 앞서 정리한 것처럼 벡터는 스칼라들의 집합입니다. 따라서 우리는 \\(k\\)개의 예측변수들과 \\(n\\)개의 관측치를 이용한 행렬들로 수식 와 같이 다중선형회귀모델을 나타낼 수 있습니다. \\[\\begin{equation} \\label{eq:4} y = \\begin{bmatrix}y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix} X = \\begin{bmatrix}1 &amp; x_{21} &amp; x_{31} &amp; \\cdots &amp; x_{k1}\\\\ 1 &amp; x_{22} &amp; x_{32} &amp; \\cdots &amp; x_{k2}\\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\ 1 &amp; x_{2n} &amp; x_{3n} &amp; \\cdots &amp; x_{kn}\\\\\\end{bmatrix} \\beta = \\begin{bmatrix} \\beta_1 \\\\ \\beta_2 \\\\ \\vdots \\\\ \\beta_n \\\\\\end{bmatrix} u = \\begin{bmatrix} u_1 \\\\ u_2 \\\\ \\vdots \\\\ u_n \\\\\\end{bmatrix} \\end{equation}\\] 수식 에 입각해 우리는 선형모델을 각각의 관측된 \\(y\\)값과 \\(X\\), 그리고 추정된 \\(\\beta\\)와 \\(u\\)를 가지로 수식 로 보여줄 수 있습니다. \\[\\begin{equation} \\label{eq:5} \\begin{split} &amp;y_1 = \\beta_1 + \\beta_2x_{21} +\\cdots+\\beta_kx_{k1} + u_1\\\\ &amp;y_2 = \\beta_1 + \\beta_2x_{22} +\\cdots+\\beta_kx_{k2} + u_2\\\\ &amp;\\vdots\\\\ &amp;y_n = \\beta_1 + \\beta_2x_{2n} +\\cdots+\\beta_kx_{kn} + u_n\\\\ \\end{split} \\end{equation}\\] 여기서 \\(X\\)와 \\(y\\)는 실제 관측된 데이터로부터 나온 예측변수와 종속변수의 개별 관측값을 의미합니다. 따라서 현실의 데이터는 항상 우리가 알지 못하는 요인들에 의해 영향을 받을 수 있기 때문에 우리는 이 ‘관측하지 못한’ 그리고 ‘비체계적인’ 요인들을 나타내는 오차항을 모델에 포함시킵니다. 그러므로 다중선형회귀모델을 보여주는 행렬은 아래의 수식 와 같이 정리할 수 있습니다. \\[\\begin{equation} \\label{eq:6} y = \\begin{bmatrix}y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix} = \\begin{bmatrix}1 &amp; x_{21} &amp; x_{31} &amp; \\cdots &amp; x_{k1}\\\\ 1 &amp; x_{22} &amp; x_{32} &amp; \\cdots &amp; x_{k2}\\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\ 1 &amp; x_{2n} &amp; x_{3n} &amp; \\cdots &amp; x_{kn}\\\\\\end{bmatrix} \\begin{bmatrix} \\beta_1 \\\\ \\beta_2 \\\\ \\vdots \\\\ \\beta_n \\\\\\end{bmatrix} + \\begin{bmatrix} u_1 \\\\ u_2 \\\\ \\vdots \\\\ u_n \\\\\\end{bmatrix} \\end{equation}\\] 그리고 예측변수 \\(X\\)의 1은 절편값을 나타냅니다. 행렬 계산 공식을 떠올리지 못하더라도 이 수식 을 직관적으로 이해해보자면, 관측된 종속변수 \\(y_i\\)는 관측된 \\(k\\)개의 예측변수의 관측치들인 \\(x_{ki}\\)가 모델에 의해 추정된 \\(\\beta\\)들과 결합하여 계산되는 예측값 \\(\\hat{y}_i\\)에 현실에서 관측하지 못한 오차들이 더해진 결과라는 것입니다. 우리는 위의 행렬을 계산해서 OLS, 즉 오차의 최소제곱합(least sum of squared errors)을 구할 수 있습니다. 이건 그냥 참고로 알아두세요. \\[\\begin{equation*} \\begin{split} S&amp;=\\Sigma^n_{i=1}\\epsilon^2_i = \\Sigma^n_{i=1}(Y_i - \\beta_1 - \\beta_2X_i1 - \\cdots - \\beta_kX_{ik})^2\\\\ &amp;=\\epsilon^{\\prime}\\epsilon = (Y - X\\beta)^{\\prime}(Y-X\\beta) = Y^{\\prime}Y - Y^{\\prime}X\\beta - \\beta^{\\prime}X^{\\prime}Y + \\beta^{\\prime}X^{\\prime}X\\beta \\end{split} \\end{equation*}\\] 이렇게 계산된 \\(S\\)에 편미분을 취하면? \\[\\begin{equation*} \\begin{split} \\frac{\\partial S}{\\partial \\beta} =&amp;-2X^{\\prime}Y + 2X^{\\prime}Y\\beta = 0\\\\ \\rightarrow&amp;X^{\\prime}X\\hat{\\beta}= X^{\\prime}Y\\\\ \\rightarrow&amp;\\hat{\\beta} = (X^{\\prime}X)^{-1}X^{\\prime}Y \\end{split} \\end{equation*}\\] 이렇게 됩니다. 이 행렬계산을 통해서 \\(\\hat{\\beta}_k\\)를 얻을 수 있고(이게 계수값이죠!), R은 이 행렬계산을 구현할 수 있습니다. 우리가 수많은 변수들과 관측치들을 가지고 있다고 해봅시다. \\(i \\leq n\\), 즉 \\(n\\)개의 관측치를 가지고 있다고 할 때, 우리는 \\[ y_i = \\alpha + \\beta_1x_{i1} + \\cdots + \\beta_{K}x_{iK} + \\epsilon_i \\] 라는 다중선형회귀모델을 구축할 수 있게 됩니다. 이는 예측변수들의 수에 절편까지를 고려한 \\(K+1\\)개의 제한된 변수를 가진 함수를 가지 모델이라는 것을 의미하며, 우리는 이 모델에서 오차의 제곱합이 최소가 되는 계수값들을 구할 수가 있게 되는 것입니다. 조금 더 직관적이게 행렬을 보여드리자면, \\[\\begin{equation*} y = \\begin{bmatrix}y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix}, x_k = \\begin{bmatrix}x_{1k} \\\\ x_{2k} \\\\ \\vdots &amp; x_{Nk}\\end{bmatrix}, \\text{ 그리고 }, \\epsilon = \\begin{bmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\\\\\end{bmatrix} \\end{equation*}\\] 이라고 할 때, 각각의 \\(y\\), \\(x_k\\), \\(\\epsilon\\)은 \\(N\\times 1\\)의 벡터입니다. 관측된 종속변수, 예측변수, 그리고 오차항이죠. 이를 선형회귀모델에 집어넣으면 아래와 같이 다시 써볼 수 있습니다. \\[\\begin{equation*} y = \\begin{bmatrix}y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix} = \\alpha + \\beta_1 \\begin{bmatrix}x_{11} \\\\ x_{21} \\\\ \\vdots &amp; x_{N1}\\end{bmatrix} + \\cdots + \\beta_k\\begin{bmatrix}x_{1K} \\\\ x_{2K} \\\\ \\vdots &amp; x_{NK}\\end{bmatrix} + v \\end{equation*}\\] 이렇게 보면 조금 더 명확하죠? 우리가 지겹도록 보았던 \\(y = \\alpha + \\beta_1x_1 + \\cdots + \\beta_kx_k + \\epsilon\\)과 형태가 같습니다. 그런데 이렇게 길게 쓰는 건 너무 비효율적이니 계수값들의 벡터를 \\(\\beta = (\\alpha\\;\\;\\beta_k\\;\\;\\cdots\\;\\; \\beta_k)^{\\prime}\\)라고 나타내고 예측변수들을 \\(\\textbf{X} = (1\\;\\;x_1\\;\\;\\cdots\\;\\; x_k)\\)라고 표현합니다. 이때 \\(\\textbf{X}\\)는 \\(N\\times(K+1)\\)의 행렬이며, \\(\\beta\\)는 \\(1\\times(K+1)\\)의 벡터입니다. 이해하기는 어렵지 않겠죠? 왜냐면 예측변수들이야 변수들의 개수(\\(K\\))에 절편을 표현할 항이 하나 더 추가되서 (\\(K+1\\))에 실제 관측된 관측치의 수(\\(N\\))을 고려해준 것이고, 계수값은 변수들에 절편을 포함한 개수만큼 계산되는데 위에서 살펴본 것처럼 열로 표현되니까 벡터라고 할 수 있는 것입니다. 그렇다면 우리는 행렬로 나타내는 OLS를 다음과 같이 심플하게 보여줄 수 있습니다. \\[ \\textbf{y} = \\textbf{X}\\beta+\\epsilon \\] 이 하나의 식이 모든 \\(N\\)개의 관측치와 \\(K\\)개의 예측변수들에 대한 정보를 함축하고 있는 것입니다. 7.1.1 행렬 OLS에서의 가우스-마르코프 당연히 행렬 OLS도 우리가 이전 챕터들에서 보았던 OLS와 동일하니만큼 OLS의 기본가정, 가우스-마르코프 가정을 \\(\\textbf{y} = \\textbf{X}\\beta+\\epsilon\\)을 이용해 보여줄 수 있습니다. 먼저 우리의 추정치로부터 잔차의 벡터를 얻어냅니다. 그냥 식의 좌변과 우변을 요리조리 옮겨주면 됩니다. \\[ \\hat{\\epsilon} = \\textbf{y}-\\textbf{X}\\hat{\\beta} \\] 개별 종속변수의 관측치에서 우리가 추정한 계수—모델에 일련의 예측변수들의 관측치들을 하나하나 대입하여 계산된 예측값(\\(\\hat{y} = \\textbf{X}\\hat{\\beta}\\))을 제해주면 나오는 것이 우리가 모델로 설명하지 못한 종속변수의 변동(variations), 오차(errors)가 될 것입니다. 물론 표본을 가지고 하는 것이니 잔차(residuals)라고 하는 게 정확한 표현입니다. 그리고 우리는 이렇게 얻은 잔차의 제곱합을 최소한으로 하고 싶은거죠. 이게 OLS의 핵심이니까요. 최소로 만드는 값을 찾는 것은 여러 가지 방법이 있겠습니다만, 여기서는 편미분을 통해 제곱합이 0이 되는 해를 구하는 방식을 취하겠습니다. \\[ \\frac{\\partial \\hat{\\epsilon}^{\\prime}\\hat{\\epsilon}}{\\partial\\hat{\\beta}} = \\frac{\\partial(\\textbf{y}-\\textbf{X}\\hat{\\beta})^{\\prime}(\\textbf{y}-\\textbf{X}\\hat{\\beta})}{\\partial \\hat{\\beta}} = 2\\textbf{X}^{\\prime}\\textbf{X}\\hat{\\beta}-2\\textbf{X}^{\\prime}\\textbf{y} = 0 \\] 여기서 \\(\\hat{\\epsilon}^{\\prime}\\hat{\\epsilon}\\)은 제곱합, \\(\\sum^{N}_{i=1}\\epsilon^2_i\\)를 벡터로 보여준 것입니다. 아무튼 $ 2{}-2{} = 0$라고 할 때, 우리는 이 식을 다시 \\(\\hat{\\beta} = (\\textbf{X}^{\\prime}\\textbf{X})^{-1}\\textbf{X}^{\\prime}\\textbf{y}\\)로 나타낼 수 있습니다. 이것이 회귀계수(물론 절편까지!)를 구하는 보다 간명한 해(solution)가 되겠습니다. 그리고 이때 회귀계수의 분산은 \\(Var(\\hat{\\beta}) = \\sigma^2(\\textbf{X}^{\\prime}\\textbf{X})^{-1}\\)가 됩니다. \\(Var(\\hat{\\beta})\\)는 \\((K+1)\\times(K+1)\\)의 차원을 갖는 행렬이 되고, 이를 우리는 분산-공분산행렬(variance-covariance matrix)이라고 합니다. 이 분산-공분산행렬은 무엇이고, 왜 중요한지를 이제부터 살펴보겠습니다. 7.1.2 표집분포 (Sampling distribution) 지겹게 들어왔던 표집분포로 다시 돌아왔습니다. 그런데 이번에는 조금 다른 방식으로 표집분포를 나타내보겠습니다. 행렬에 대해서 살펴보았으니 표집분포를 행렬의 형태로 나타내 보겠습니다. "]
]
